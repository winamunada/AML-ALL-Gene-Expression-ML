# -*- coding: utf-8 -*-
"""AML_ALL_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZNKZQ4D6h2-tV44u0ZrFRjY9Vq1Y6XB5

# Evaluating Machine Learning Models for Gene Expression Analysis in AML and ALL Cancer Classification and Clustering

## Machine Learning Project
## Wina Munada - AIU221063
"""

from google.colab import drive
drive.mount('/content/drive')

"""#### Import all the necessary library"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import random
import xgboost as xgb
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC

params = {'legend.fontsize': 'x-large',
          'figure.figsize': (15, 10),
         'axes.labelsize': 'x-large',
         'axes.titlesize':'x-large',
         'xtick.labelsize':'x-large',
         'ytick.labelsize':'x-large'}
plt.rcParams.update(params)
sns.set_theme(style="darkgrid")
np.random.seed(42)

warnings.filterwarnings("ignore")

le = LabelEncoder()

"""### Load data"""

train_data = pd.read_csv("/content/drive/MyDrive/gene-expression/data_set_ALL_AML_train.csv")
test_data = pd.read_csv("/content/drive/MyDrive/gene-expression/data_set_ALL_AML_independent.csv")
labels = pd.read_csv("/content/drive/MyDrive/gene-expression/actual.csv", index_col = 'patient')

train_data.head()

test_data.head()

"""### Check NAs"""

print(train_data.isna().sum().sum())
print(test_data.isna().sum().sum())

"""## EDA and data preprocessing

It seems like **call** columns have "A" almost everywhere, so I will drop it.
"""

cols_train = [col for col in train_data.columns if "call" in col]
cols_test = [col for col in test_data.columns if "call" in col]

train_data.drop(cols_train, axis=1, inplace=True)
test_data.drop(cols_test, axis=1, inplace=True)

"""Here we have features in rows and patients in cols, so we need to transpose data."""

train_data = train_data.T
test_data = test_data.T

train_data.head()

train_data.columns = test_data.iloc[1].values
train_data.drop(["Gene Description", "Gene Accession Number"], axis=0, inplace=True)
test_data.columns = test_data.iloc[1].values
test_data.drop(["Gene Description", "Gene Accession Number"], axis=0, inplace=True)

train_data.head()

# Adding new column
train_data["patient"] = train_data.index.values
test_data["patient"] = test_data.index.values

train_data.head()

train_data = train_data.astype("int32")
test_data = test_data.astype("int32")

labels["cancer"] = le.fit_transform(labels["cancer"])
train_data = pd.merge(train_data, labels, on="patient")
test_data = pd.merge(test_data, labels, on="patient")

train_data["cancer"].value_counts()

test_data["cancer"].value_counts()

fig, axs = plt.subplots(1, 2)
sns.countplot(x="cancer", data=train_data, ax=axs[0])
axs[0].set_title("Train data", fontsize=24)
sns.countplot(x="cancer", data=test_data, ax=axs[1])
axs[1].set_title("Test data", fontsize=24)
plt.show()

"""In test data we have $\frac{ALL}{AML}$ ratio about $\frac{20}{14}=1.43$ and in train $\frac{27}{11}=2.45$. Lets use upsampling to combat class imbalance. I think here we can add about 8 additional random samples of **AML** class."""

upsampled_data = random.sample(train_data.query("cancer == 1")["patient"].index.to_list(), k=8, )

upsampled_data

train_data_upsampled = pd.concat([train_data, train_data.iloc[upsampled_data, :]])

fig, axs = plt.subplots(1, 2)
sns.countplot(x="cancer", data=train_data_upsampled, ax=axs[0])
axs[0].set_title("Train data", fontsize=24)
sns.countplot(x="cancer", data=test_data, ax=axs[1])
axs[1].set_title("Test data", fontsize=24)
fig.suptitle("After upsampling", fontsize=24)
plt.show()

"""Scaling the data."""

X_train = train_data_upsampled.drop(columns=["patient", "cancer"])
y_train = train_data_upsampled["cancer"]
X_test = test_data.drop(columns=["patient", "cancer"])
y_test = test_data["cancer"]

# Features scaling
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

# PCA transformation to reduce the dimensionality
pca = PCA(n_components=2)
reduced_train = pca.fit_transform(X_train_scaled)
reduced_test = pca.transform(X_test_scaled)

pca = PCA()
pca.fit_transform(X_train_scaled)
total = sum(pca.explained_variance_)
k = 0
current_variance = 0
while current_variance / total < 0.90:
    current_variance += pca.explained_variance_[k]
    k = k + 1
print(k, " features explain around 90% of the variance. From 7129 features to ", k, sep='')

pca = PCA(n_components=k)
X_train_pca = pca.fit(X_train_scaled)
X_train_pca = pca.transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

var_exp = pca.explained_variance_ratio_.cumsum()
var_exp = var_exp*100
plt.bar(range(1, k + 1), var_exp, color="brown")
plt.xlabel("Cumulutive explained variance", fontsize=18)
plt.ylabel("Number of components", fontsize=18)
plt.xlim((0.5, k + 1))
plt.show()

"""Here we can take approximately 27 PC for downstream analysis. May be we will use them later.

## Classification

I will start with **random forest**, esimate feature importance and then try to build ensemble using different classifiers.

## Random forest
"""

rf_params = {"bootstrap": [False, True],
             "n_estimators": [60, 70, 80, 90, 100],
             "max_features": [0.6, 0.65, 0.7, 0.75, 0.8],
             "min_samples_leaf": [8, 10, 12, 14],
             "min_samples_split": [3, 5, 7]
        }

rf_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_params, scoring="f1")
rf_search.fit(X_train_scaled, y_train)
best_rf = rf_search.best_estimator_

best_rf

rf_prediction = best_rf.predict(X_test_scaled)


f1_score = metrics.f1_score(y_test, rf_prediction)
print('Validation f1-score of RandomForest Classifier is', f1_score)
print ("\nClassification report :\n", metrics.classification_report(y_test, rf_prediction))

# Confusion matrix
plt.figure(figsize=(18, 14))
plt.subplot(221)
sns.heatmap(metrics.confusion_matrix(y_test, rf_prediction), annot=True, fmt = "d", linecolor="k", linewidths=3)
plt.title("CONFUSION MATRIX", fontsize=20)

# ROC curve
rf_predicted_probs = best_rf.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_predicted_probs)
plt.subplot(222)
plt.plot(fpr, tpr, label = ("Area_under the curve :", metrics.auc(fpr, tpr)), color = "r")
plt.plot([1,0], [1,0], linestyle = "dashed", color ="k")
plt.legend(loc = "best")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC - CURVE & AREA UNDER CURVE",fontsize=20)
plt.show()

print(f"Quantity of features with 0 importance: {(best_rf.feature_importances_ == 0).sum()}")

"""We have a lot of unimportant genes, so we can look only at non null features (with 0 importance)."""

mask = (best_rf.feature_importances_ != 0)
importances = best_rf.feature_importances_[mask]
feature_names = train_data.columns.values[:7129][mask]

fig = plt.figure(figsize=(18, 12))
ax = fig.add_subplot(111)
sns.barplot(x=feature_names, y=importances)
plt.ylabel("Feature importance")
plt.xlabel("Feature name")
plt.title("Feature importance", fontsize=28)
ax.set_xticklabels(feature_names, rotation = 45)
plt.show()

"""As we can see there are also lots of features with pretty low importance (about e-5), but also we can admit 4 genes, wich have a big contribution in cancer classification. So genes **X95735**, **M55150**, and **M27891** contribute the most to the differences between **ALL** and **AML** cancer types in accordance with random forest classifier."""

print(metrics.accuracy_score(y_test, rf_prediction))

"""It is a good model, but I am going to build ensemle based on different classifiers and hope it can show better performance.

## Ensemble

I want to build 5 different classifiers, evaluate some metrics (f1 score) and then try to create ensemble.

## KNN
"""

knn_params = {
    "n_neighbors": [i for i in range(1, 30, 5)],
    "weights": ["uniform", "distance"],
    "algorithm": ["kd_tree"],
    "leaf_size": [1, 10, 20, 30],
    "p": [1, 2]
}
knn_search = GridSearchCV(KNeighborsClassifier(), knn_params, n_jobs=-1, verbose=1, scoring="f1")
knn_search.fit(X_train_scaled, y_train)
best_knn = knn_search.best_estimator_

best_knn

# KNN prediction
knn_prediction = best_knn.predict(X_test_scaled)

# F1 Score
knn_f1_score = metrics.f1_score(y_test, knn_prediction)
print('Validation F1-score of KNN Classifier is', knn_f1_score)
print("\nClassification report :\n", metrics.classification_report(y_test, knn_prediction))

# Confusion Matrix
plt.figure(figsize=(18, 14))

plt.subplot(221)
sns.heatmap(metrics.confusion_matrix(y_test, knn_prediction), annot=True, fmt="d", linecolor="k", linewidths=3)
plt.title("CONFUSION MATRIX", fontsize=20)

# ROC Curve
knn_predicted_probs = best_knn.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, knn_predicted_probs)
plt.subplot(222)
plt.plot(fpr, tpr, label=("Area under the curve:", metrics.auc(fpr, tpr)), color="r")
plt.plot([1, 0], [1, 0], linestyle="dashed", color="k")
plt.legend(loc="best")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC - CURVE & AREA UNDER CURVE", fontsize=20)
plt.show()

"""best_knn = KNeighborsClassifier(algorithm='kd_tree', leaf_size=1, n_neighbors=6, p=1,
                     weights='distance')

## Logistic Regression
"""

lr_params = {"C": [1e-03, 1e-2, 1e-1, 1, 10],
      "penalty": ["l1", "l2"]}
log_refr_search = GridSearchCV(estimator=LogisticRegression(), param_grid=lr_params, scoring="f1")
log_refr_search.fit(X_train_scaled, y_train)
best_lr = log_refr_search.best_estimator_

best_lr

"""best_lr = LogisticRegression(C=0.001)"""

# Logistic Regression prediction
lr_prediction = best_lr.predict(X_test_scaled)

# F1 Score
lr_f1_score = metrics.f1_score(y_test, lr_prediction)
print('Validation F1-score of Logistic Regression Classifier is', lr_f1_score)
print("\nClassification report :\n", metrics.classification_report(y_test, lr_prediction))

# Confusion Matrix
plt.figure(figsize=(18, 14))

plt.subplot(221)
sns.heatmap(metrics.confusion_matrix(y_test, lr_prediction), annot=True, fmt="d", linecolor="k", linewidths=3)
plt.title("CONFUSION MATRIX", fontsize=20)

# ROC Curve
lr_predicted_probs = best_lr.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, lr_predicted_probs)
plt.subplot(222)
plt.plot(fpr, tpr, label=("Area under the curve:", metrics.auc(fpr, tpr)), color="r")
plt.plot([1, 0], [1, 0], linestyle="dashed", color="k")
plt.legend(loc="best")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC - CURVE & AREA UNDER CURVE", fontsize=20)
plt.show()

"""## Decision Tree"""

dt_params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4, 5, 6], 'max_depth':[3,4,5,6,7,8]}
dst_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), param_grid=dt_params, verbose=1, cv=3, scoring="f1")
dst_search.fit(X_train_scaled, y_train)
best_dt = dst_search.best_estimator_

best_dt

# Decision Tree prediction
dt_prediction = best_dt.predict(X_test_scaled)

# F1 Score
dt_f1_score = metrics.f1_score(y_test, dt_prediction)
print('Validation F1-score of Decision Tree Classifier is', dt_f1_score)
print("\nClassification report :\n", metrics.classification_report(y_test, dt_prediction))

# Confusion Matrix
plt.figure(figsize=(18, 14))

plt.subplot(221)
sns.heatmap(metrics.confusion_matrix(y_test, dt_prediction), annot=True, fmt="d", linecolor="k", linewidths=3)
plt.title("CONFUSION MATRIX", fontsize=20)

# ROC Curve
dt_predicted_probs = best_dt.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, dt_predicted_probs)
plt.subplot(222)
plt.plot(fpr, tpr, label=("Area under the curve:", metrics.auc(fpr, tpr)), color="r")
plt.plot([1, 0], [1, 0], linestyle="dashed", color="k")
plt.legend(loc="best")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC - CURVE & AREA UNDER CURVE", fontsize=20)
plt.show()

"""best_dt = DecisionTreeClassifier(max_depth=3, max_leaf_nodes=2, random_state=42)"""

print(f"Quantity of features with 0 importance: {(best_dt.feature_importances_ == 0).sum()}")

mask_dt = (best_dt.feature_importances_ != 0)
importances_dt = best_dt.feature_importances_[mask_dt]
feature_names_dt = train_data.columns.values[:7129][mask_dt]

fig = plt.figure(figsize=(18, 12))
ax = fig.add_subplot(111)
sns.barplot(x=feature_names_dt, y=importances_dt)
plt.ylabel("Feature importance")
plt.xlabel("Feature name")
plt.title("Feature importance", fontsize=28)
ax.set_xticklabels(feature_names_dt, rotation = 45)
plt.show()

"""## SVC"""

svc_params = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},
              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]
svc_search = GridSearchCV(SVC(probability=True), param_grid=svc_params, n_jobs=-1, verbose=1, scoring="f1")
svc_search.fit(X_train_scaled, y_train)
best_svc = svc_search.best_estimator_

best_svc

# SVC prediction
svc_prediction = best_svc.predict(X_test_scaled)

# F1 Score
svc_f1_score = metrics.f1_score(y_test, svc_prediction)
print('Validation F1-score of SVC Classifier is', svc_f1_score)
print("\nClassification report :\n", metrics.classification_report(y_test, svc_prediction))

# Confusion Matrix
plt.figure(figsize=(18, 14))

plt.subplot(221)
sns.heatmap(metrics.confusion_matrix(y_test, svc_prediction), annot=True, fmt="d", linecolor="k", linewidths=3)
plt.title("CONFUSION MATRIX", fontsize=20)

# ROC Curve
svc_predicted_probs = best_svc.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, svc_predicted_probs)
plt.subplot(222)
plt.plot(fpr, tpr, label=("Area under the curve:", metrics.auc(fpr, tpr)), color="r")
plt.plot([1, 0], [1, 0], linestyle="dashed", color="k")
plt.legend(loc="best")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC - CURVE & AREA UNDER CURVE", fontsize=20)
plt.show()

"""best_svc = SVC(C=1, kernel='linear', probability=True)

## Builing an ensemble

So now we have 4 different classifiers, some of them are good and the others are not and I want to use all of them to classify two cancer types. I will weigh my classifiers based on f1-score such that I will calculate mean f1-score and then divide f1-score of each model by mean, so the best estimator will have the higher weight.
"""

rf_f1_score = metrics.f1_score(y_test, best_rf.predict(X_test_scaled))
knn_f1_score = metrics.f1_score(y_test, best_knn.predict(X_test_scaled))
lr_f1_score = metrics.f1_score(y_test, best_lr.predict(X_test_scaled))
dt_f1_score = metrics.f1_score(y_test, best_dt.predict(X_test_scaled))
svc_f1_score = metrics.f1_score(y_test, best_svc.predict(X_test_scaled))
kmeans_f1_score = f1_score(y_test, y_pred_kmeans)

# Calculate metrics for each model

svc_recall = metrics.recall_score(y_test, svc_prediction)
svc_accuracy = metrics.accuracy_score(y_test, svc_prediction)

knn_recall = metrics.recall_score(y_test, knn_prediction)
knn_accuracy = metrics.accuracy_score(y_test, knn_prediction)

lr_recall = metrics.recall_score(y_test, lr_prediction)
lr_accuracy = metrics.accuracy_score(y_test, lr_prediction)

dt_recall = metrics.recall_score(y_test, dt_prediction)
dt_accuracy = metrics.accuracy_score(y_test, dt_prediction)

rf_recall = metrics.recall_score(y_test, rf_prediction)
rf_accuracy = metrics.accuracy_score(y_test, rf_prediction)

kmeans_recall = recall_score(y_test, y_pred_kmeans)
kmeans_accuracy = accuracy_score(y_test, y_pred_kmeans)

# Create a DataFrame with all metrics
model_metrics = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'K-Means'],
    'F1-Score': [svc_f1_score, knn_f1_score, lr_f1_score, dt_f1_score, rf_f1_score, kmeans_f1_score],
    'Recall': [svc_recall, knn_recall, lr_recall, dt_recall, rf_recall, kmeans_recall],
    'Accuracy': [svc_accuracy, knn_accuracy, lr_accuracy, dt_accuracy, rf_accuracy, kmeans_accuracy],
})

# Sort by F1-Score
sorted_model_metrics = model_metrics.sort_values(by='F1-Score', ascending=False)

print(sorted_model_metrics)

mean_f1_score = np.mean([rf_f1_score, knn_f1_score, lr_f1_score, dt_f1_score, svc_f1_score])
weight_rf = rf_f1_score / mean_f1_score
weight_knn = knn_f1_score / mean_f1_score
weight_lr = lr_f1_score / mean_f1_score
weight_dt = dt_f1_score / mean_f1_score
weight_svc = svc_f1_score / mean_f1_score

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming the metrics have already been calculated as shown in the previous code
# Create a DataFrame with all metrics
model_metrics = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'K-Means'],
    'F1-Score': [svc_f1_score, knn_f1_score, lr_f1_score, dt_f1_score, rf_f1_score, kmeans_f1_score],
    'Recall': [svc_recall, knn_recall, lr_recall, dt_recall, rf_recall, kmeans_recall],
    'Accuracy': [svc_accuracy, knn_accuracy, lr_accuracy, dt_accuracy, rf_accuracy, kmeans_accuracy],
})

# Sort by F1-Score
sorted_model_metrics = model_metrics.sort_values(by='F1-Score', ascending=False)

# Melt the DataFrame for seaborn
melted_metrics = pd.melt(sorted_model_metrics, id_vars=['Model'], var_name='Metrics', value_name='Score')

# Plotting
plt.figure(figsize=(10, 6))
sns.set(style="darkgrid")

sns.lineplot(data=melted_metrics, x='Metrics', y='Score', hue='Model', marker='o')

plt.title('Model Performance Metrics')
plt.xlabel('Metrics')
plt.ylabel('Score')
plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

model_weights = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Decision Tree',
              'Random Forest'],

    'weight': [weight_svc, weight_knn, weight_lr, weight_dt,
              weight_rf]})
model_weights.sort_values(by='weight', ascending=False)

ensemble = VotingClassifier(estimators=[("rf", best_rf), ("knn", best_knn), ("lr", best_lr), ("dt", best_dt), ("svc", best_svc)],
                                        voting="soft", weights=[weight_rf, weight_knn, weight_lr, weight_dt, weight_svc])

ensemble.fit(X_train_scaled, y_train)

ens_prediction = ensemble.predict(X_test_scaled)
f1_score = metrics.f1_score(y_test, ens_prediction)
print('Validation f1-score of Ensemble Classifier is', f1_score)
print ("\nClassification report :\n", metrics.classification_report(y_test, ens_prediction))

# Confusion matrix
plt.figure(figsize=(18, 14))
plt.subplot(221)
sns.heatmap(metrics.confusion_matrix(y_test, ens_prediction), annot=True, fmt = "d", linecolor="k", linewidths=3)
plt.title("CONFUSION MATRIX", fontsize=20)

# ROC curve
ens_predicted_probs = ensemble.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = metrics.roc_curve(y_test, ens_predicted_probs)
plt.subplot(222)
plt.plot(fpr, tpr, label = ("Area_under the curve :", metrics.auc(fpr, tpr)), color = "r")
plt.plot([1,0], [1,0], linestyle = "dashed", color ="k")
plt.legend(loc = "best")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC - CURVE & AREA UNDER CURVE",fontsize=20)
plt.show()

"""As we can see Ensemble have shown the best performance"""